"use strict";(self.webpackChunksquirrels_docs=self.webpackChunksquirrels_docs||[]).push([[7064],{2209:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>c});var r=n(5674),s=n(4848),i=n(8453);const l={slug:"1brc-postgres",title:"Query 1B Rows in PostgreSQL >25x Faster with Squirrels!",authors:"thuang",tags:["squirrels","postgresql","duckdb","performance"]},a=void 0,o={authorsImageUrls:[void 0]},c=[{value:"The Challenge",id:"the-challenge",level:2}];function u(e){const t={a:"a",h2:"h2",li:"li",ol:"ol",p:"p",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(t.a,{href:"https://www.morling.dev/blog/one-billion-row-challenge/",children:"One Billion Row Challenge"})," has been making waves in the data engineering community lately. Originally created to test CSV parsing performance, the challenge involves processing a file containing 1 billion weather measurements to calculate basic temperature statistics for each city. In this post, I'll tackle a variation of this challenge using PostgreSQL and demonstrate how to achieve dramatic performance improvements using Squirrels."]}),"\n",(0,s.jsx)(t.h2,{id:"the-challenge",children:"The Challenge"}),"\n",(0,s.jsxs)(t.p,{children:["The original ",(0,s.jsx)(t.a,{href:"https://www.morling.dev/blog/one-billion-row-challenge/",children:"One Billion Row Challenge"})," focuses on raw CSV processing performance. For our variation, we'll:"]}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsx)(t.li,{children:"Load 1 billion rows into PostgreSQL with additional columns"}),"\n",(0,s.jsx)(t.li,{children:"Query for city-level temperature statistics"}),"\n",(0,s.jsx)(t.li,{children:"Create a Squirrels project to serve these analytics via REST API"}),"\n",(0,s.jsx)(t.li,{children:"Demonstrate significant query performance improvements"}),"\n",(0,s.jsx)(t.li,{children:"Show how to handle incremental data updates"}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>a});var r=n(6540);const s={},i=r.createContext(s);function l(e){const t=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),r.createElement(i.Provider,{value:t},e.children)}},5674:e=>{e.exports=JSON.parse('{"permalink":"/blog/1brc-postgres","source":"@site/blog/2024-12-09-1brc-postgres.md","title":"Query 1B Rows in PostgreSQL >25x Faster with Squirrels!","description":"The One Billion Row Challenge has been making waves in the data engineering community lately. Originally created to test CSV parsing performance, the challenge involves processing a file containing 1 billion weather measurements to calculate basic temperature statistics for each city. In this post, I\'ll tackle a variation of this challenge using PostgreSQL and demonstrate how to achieve dramatic performance improvements using Squirrels.","date":"2024-12-09T00:00:00.000Z","tags":[{"inline":true,"label":"squirrels","permalink":"/blog/tags/squirrels"},{"inline":true,"label":"postgresql","permalink":"/blog/tags/postgresql"},{"inline":true,"label":"duckdb","permalink":"/blog/tags/duckdb"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":5.865,"hasTruncateMarker":true,"authors":[{"name":"Tim Huang","title":"Co-Founder of Squirrels Analytics","url":"https://github.com/ty2huang","imageURL":"https://github.com/ty2huang.png","key":"thuang","page":null}],"frontMatter":{"slug":"1brc-postgres","title":"Query 1B Rows in PostgreSQL >25x Faster with Squirrels!","authors":"thuang","tags":["squirrels","postgresql","duckdb","performance"]},"unlisted":false,"nextItem":{"title":"Squirrels - What Problems Does It Solve?","permalink":"/blog/problems-solved"}}')}}]);